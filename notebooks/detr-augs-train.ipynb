{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8691613,"sourceType":"datasetVersion","datasetId":5211891},{"sourceId":8694291,"sourceType":"datasetVersion","datasetId":5213860},{"sourceId":8691008,"sourceType":"datasetVersion","datasetId":5211432}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":14712.43423,"end_time":"2024-06-15T08:04:35.092137","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-15T03:59:22.657907","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport yaml\nimport os\nimport json\nimport torch\n\nfrom IPython.display import clear_output\nfrom PIL import Image","metadata":{"papermill":{"duration":3.565509,"end_time":"2024-06-15T03:59:29.000178","exception":false,"start_time":"2024-06-15T03:59:25.434669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:12.987260Z","iopub.execute_input":"2024-06-15T12:01:12.988248Z","iopub.status.idle":"2024-06-15T12:01:12.994278Z","shell.execute_reply.started":"2024-06-15T12:01:12.988202Z","shell.execute_reply":"2024-06-15T12:01:12.993108Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self, params: dict):\n\n        for key, value in params.items():\n            setattr(self, key, value)\n        \n        print('Config ready')","metadata":{"papermill":{"duration":0.013461,"end_time":"2024-06-15T03:59:44.258561","exception":false,"start_time":"2024-06-15T03:59:44.245100","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:12.996632Z","iopub.execute_input":"2024-06-15T12:01:12.997031Z","iopub.status.idle":"2024-06-15T12:01:13.003251Z","shell.execute_reply.started":"2024-06-15T12:01:12.996998Z","shell.execute_reply":"2024-06-15T12:01:13.002309Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **Constants for notebook**","metadata":{"papermill":{"duration":0.004999,"end_time":"2024-06-15T03:59:44.269055","exception":false,"start_time":"2024-06-15T03:59:44.264056","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nHOME = os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:01:13.004853Z","iopub.execute_input":"2024-06-15T12:01:13.005202Z","iopub.status.idle":"2024-06-15T12:01:13.010791Z","shell.execute_reply.started":"2024-06-15T12:01:13.005170Z","shell.execute_reply":"2024-06-15T12:01:13.009783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:01:13.013452Z","iopub.execute_input":"2024-06-15T12:01:13.014076Z","iopub.status.idle":"2024-06-15T12:01:26.068847Z","shell.execute_reply.started":"2024-06-15T12:01:13.014043Z","shell.execute_reply":"2024-06-15T12:01:26.067801Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.2.32)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.82)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.1)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.2.8)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -P {HOME} -q https://github.com/ultralytics/assets/releases/download/v8.2.0/rtdetr-x.pt","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:01:26.070263Z","iopub.execute_input":"2024-06-15T12:01:26.070558Z","iopub.status.idle":"2024-06-15T12:01:27.809445Z","shell.execute_reply.started":"2024-06-15T12:01:26.070531Z","shell.execute_reply":"2024-06-15T12:01:27.808346Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Train params\nMODEL_NAME = './rtdetr-x.pt'\nIMAGE_SIZE = 640\nBATCH_SIZE = 8\nNUM_EPOCHS = 100\nYAML_PATH = ''\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n# Generate YAML file params\nRELATIVE_TRAIN_PATH = '/kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/images/train' # Relative to 'PATH_TO_DATASET'\nRELATIVE_TEST_PATH =  ''# Relative to 'PATH_TO_DATASET'\nREALATIVE_VAL_PATH = '/kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/images/val' # Relative to 'PATH_TO_DATASET'\nDICT_WITH_CLASS_NAMES = [\"adj\", \"int\", \"geo\", \"pro\", \"non\"]","metadata":{"papermill":{"duration":0.056005,"end_time":"2024-06-15T03:59:44.348104","exception":false,"start_time":"2024-06-15T03:59:44.292099","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:27.810932Z","iopub.execute_input":"2024-06-15T12:01:27.811236Z","iopub.status.idle":"2024-06-15T12:01:27.817660Z","shell.execute_reply.started":"2024-06-15T12:01:27.811208Z","shell.execute_reply":"2024-06-15T12:01:27.816644Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"params = {'model_name': MODEL_NAME,\n          'image_size': IMAGE_SIZE,\n          'batch_size': BATCH_SIZE,\n          'num_epochs': NUM_EPOCHS,\n          'yaml_path': YAML_PATH,\n          'device': DEVICE}\n\nconfig = Config(params=params)","metadata":{"papermill":{"duration":0.013336,"end_time":"2024-06-15T03:59:44.367108","exception":false,"start_time":"2024-06-15T03:59:44.353772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:27.818870Z","iopub.execute_input":"2024-06-15T12:01:27.819214Z","iopub.status.idle":"2024-06-15T12:01:27.827153Z","shell.execute_reply.started":"2024-06-15T12:01:27.819179Z","shell.execute_reply":"2024-06-15T12:01:27.826172Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Config ready\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import RTDETR","metadata":{"execution":{"iopub.status.busy":"2024-06-15T12:01:27.828220Z","iopub.execute_input":"2024-06-15T12:01:27.828463Z","iopub.status.idle":"2024-06-15T12:01:27.835527Z","shell.execute_reply.started":"2024-06-15T12:01:27.828441Z","shell.execute_reply":"2024-06-15T12:01:27.834685Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = RTDETR(config.model_name)","metadata":{"papermill":{"duration":1.137162,"end_time":"2024-06-15T03:59:45.509731","exception":false,"start_time":"2024-06-15T03:59:44.372569","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:27.836713Z","iopub.execute_input":"2024-06-15T12:01:27.837024Z","iopub.status.idle":"2024-06-15T12:01:28.132151Z","shell.execute_reply.started":"2024-06-15T12:01:27.836992Z","shell.execute_reply":"2024-06-15T12:01:28.131101Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# **Data**\n\nIf needed we can create yaml file\n\nDataset structure for train YOLO:","metadata":{"papermill":{"duration":0.00618,"end_time":"2024-06-15T03:59:45.523044","exception":false,"start_time":"2024-06-15T03:59:45.516864","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"```\ndataset/\r\n‚îú‚îÄ‚îÄ train/\r\n‚îÇ   ‚îú‚îÄ‚îÄ images/\r\n‚îÇ   ‚îî‚îÄ‚îÄ labels/\r\n‚îî‚îÄ‚îÄ val/\r\n    ‚îú‚îÄ‚îÄ images/\r\n    ‚îî‚îÄ‚îÄ labels/\n```","metadata":{"papermill":{"duration":0.00638,"end_time":"2024-06-15T03:59:45.535412","exception":false,"start_time":"2024-06-15T03:59:45.529032","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"labels - txt file for each image with content ```class box_center_x_n box_center_y_n wn hn```","metadata":{"papermill":{"duration":0.005886,"end_time":"2024-06-15T03:59:45.547250","exception":false,"start_time":"2024-06-15T03:59:45.541364","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import yaml","metadata":{"papermill":{"duration":0.012948,"end_time":"2024-06-15T03:59:45.567143","exception":false,"start_time":"2024-06-15T03:59:45.554195","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:28.135124Z","iopub.execute_input":"2024-06-15T12:01:28.135410Z","iopub.status.idle":"2024-06-15T12:01:28.140780Z","shell.execute_reply.started":"2024-06-15T12:01:28.135387Z","shell.execute_reply":"2024-06-15T12:01:28.139885Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"content = {\n    #'path': PATH_TO_DATASET, \n    'train': RELATIVE_TRAIN_PATH,\n    #'test': RELATIVE_TEST_PATH,\n    'val': REALATIVE_VAL_PATH,\n    'nc': len(DICT_WITH_CLASS_NAMES),\n    'names': DICT_WITH_CLASS_NAMES\n}\n\nwith open('./data.yaml', 'w') as file:\n    yaml.dump(content, file, default_flow_style=False)\n\nprint(\"data.yaml file created successfully.\")","metadata":{"papermill":{"duration":0.016168,"end_time":"2024-06-15T03:59:45.589479","exception":false,"start_time":"2024-06-15T03:59:45.573311","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:28.141810Z","iopub.execute_input":"2024-06-15T12:01:28.142136Z","iopub.status.idle":"2024-06-15T12:01:28.152053Z","shell.execute_reply.started":"2024-06-15T12:01:28.142110Z","shell.execute_reply":"2024-06-15T12:01:28.151077Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"data.yaml file created successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Train**","metadata":{"papermill":{"duration":0.005926,"end_time":"2024-06-15T03:59:45.601584","exception":false,"start_time":"2024-06-15T03:59:45.595658","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.train(data='/kaggle/working/data.yaml',\n            imgsz=config.image_size,\n            epochs=config.num_epochs,\n            batch=config.batch_size,\n            device=config.device,\n            name='Svarka',\n            degrees=5,\n            hsv_h=0.03,\n            hsv_s=1,\n            hsv_v=0.8,\n            mosaic=1,\n            flipud=0.5,\n            erasing=0.4,\n            translate=0,\n            scale=0, \n            mixup=0.1,\n            copy_paste=0.6,\n           )","metadata":{"papermill":{"duration":14668.407419,"end_time":"2024-06-15T08:04:14.033930","exception":false,"start_time":"2024-06-15T03:59:45.626511","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T12:01:28.153272Z","iopub.execute_input":"2024-06-15T12:01:28.153624Z","iopub.status.idle":"2024-06-15T16:52:51.159689Z","shell.execute_reply.started":"2024-06-15T12:01:28.153599Z","shell.execute_reply":"2024-06-15T16:52:51.157275Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.32 üöÄ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=./rtdetr-x.pt, data=/kaggle/working/data.yaml, epochs=100, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=Svarka2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.03, hsv_s=1, hsv_v=0.8, degrees=5, translate=0, scale=0, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=1, mixup=0.1, copy_paste=0.6, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/Svarka2\nOverriding model.yaml nc=80 with nc=5\nWARNING ‚ö†Ô∏è no model scale passed. Assuming scale='x'.\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1     25792  ultralytics.nn.modules.block.HGStem          [3, 32, 64]                   \n  1                  -1  6    259200  ultralytics.nn.modules.block.HGBlock         [64, 64, 128, 3, 6]           \n  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n  3                  -1  6   1248256  ultralytics.nn.modules.block.HGBlock         [128, 128, 512, 3, 6]         \n  4                  -1  6   1788928  ultralytics.nn.modules.block.HGBlock         [512, 128, 512, 3, 6, False, True]\n  5                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n  6                  -1  6   2079232  ultralytics.nn.modules.block.HGBlock         [512, 256, 1024, 5, 6, True, False]\n  7                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n  8                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n  9                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n 10                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n 11                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n 12                  -1  6   8221696  ultralytics.nn.modules.block.HGBlock         [1024, 512, 2048, 5, 6, True, False]\n 13                  -1  6   9794560  ultralytics.nn.modules.block.HGBlock         [2048, 512, 2048, 5, 6, True, True]\n 14                  -1  1    787200  ultralytics.nn.modules.conv.Conv             [2048, 384, 1, 1, None, 1, 1, False]\n 15                  -1  1   2168192  ultralytics.nn.modules.transformer.AIFI      [384, 2048, 8]                \n 16                  -1  1    148224  ultralytics.nn.modules.conv.Conv             [384, 384, 1, 1]              \n 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 18                  10  1    393984  ultralytics.nn.modules.conv.Conv             [1024, 384, 1, 1, None, 1, 1, False]\n 19            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n 21                  -1  1    148224  ultralytics.nn.modules.conv.Conv             [384, 384, 1, 1]              \n 22                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 23                   4  1    197376  ultralytics.nn.modules.conv.Conv             [512, 384, 1, 1, None, 1, 1, False]\n 24            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 25                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n 26                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 27            [-1, 21]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 28                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n 29                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 30            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 31                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n 32        [25, 28, 31]  1   7410431  ultralytics.nn.modules.head.RTDETRDecoder    [5, [384, 384, 384]]          \nrt-detr-x summary: 867 layers, 67313727 parameters, 67313727 gradients, 232.4 GFLOPs\n\nTransferred 1226/1241 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/Svarka2', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240615_120140-9t173k41</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lightsource/YOLOv8/runs/9t173k41' target=\"_blank\">Svarka2</a></strong> to <a href='https://wandb.ai/lightsource/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lightsource/YOLOv8' target=\"_blank\">https://wandb.ai/lightsource/YOLOv8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lightsource/YOLOv8/runs/9t173k41' target=\"_blank\">https://wandb.ai/lightsource/YOLOv8/runs/9t173k41</a>"},"metadata":{}},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 69.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels/train... 1600 images, 114 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [00:10<00:00, 153.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels/val... 434 images, 32 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 434/434 [00:02<00:00, 152.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels is not writeable, cache not saved.\nPlotting labels to runs/detect/Svarka2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 193 weight(decay=0.0), 256 weight(decay=0.0005), 276 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/Svarka2\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      1/100      10.7G      1.334      3.081     0.5803         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:44<00:00,  1.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:14<00:00,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        nan       0.32     0.0915     0.0319\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      2/100      10.7G     0.8889     0.8363     0.3059         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:39<00:00,  1.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        nan      0.277     0.0989     0.0352\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      3/100      10.7G     0.8583     0.7873     0.2881         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        nan      0.572      0.298      0.102\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      4/100      10.7G     0.8226     0.7959     0.2721         29        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.522      0.471      0.437      0.174\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      5/100      10.6G     0.7812     0.7484      0.253         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.632      0.578      0.584      0.236\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      6/100      10.7G     0.7545     0.7323     0.2437         35        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.702      0.618      0.647      0.263\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      7/100      10.6G      0.747     0.6837     0.2369         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.746      0.674      0.714      0.316\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      8/100      10.7G     0.7286     0.6853     0.2327         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.761      0.646      0.691      0.289\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      9/100      10.7G     0.7102     0.6903     0.2278         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.762      0.694       0.75      0.325\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     10/100      10.6G     0.6937      0.693      0.219         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.76      0.668      0.715      0.315\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     11/100      10.6G      0.707     0.6605     0.2178         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.783      0.742      0.794       0.36\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     12/100      10.6G     0.6644     0.6331      0.203         24        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.762      0.732      0.769      0.353\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     13/100      10.7G     0.6737     0.6353     0.2048         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.821       0.75      0.814      0.376\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     14/100      10.7G     0.6592     0.6266     0.2018         44        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.801      0.759      0.825      0.382\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     15/100      10.7G     0.6518     0.6147     0.1953         44        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.826      0.757      0.803      0.375\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     16/100      10.7G     0.6539     0.6111     0.1991         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.82      0.756      0.808      0.379\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     17/100      10.7G      0.639     0.6173     0.1921         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.816      0.799      0.827      0.389\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     18/100      10.7G     0.6347     0.5961     0.1917         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.852       0.79      0.846      0.402\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     19/100      10.7G     0.6186     0.5863     0.1824         26        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.84      0.786      0.841      0.409\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     20/100      10.7G     0.5966     0.5822     0.1751         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.855      0.822      0.864      0.421\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     21/100      10.6G     0.5996     0.5789     0.1771         18        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.849      0.799       0.85      0.416\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     22/100      10.7G     0.6055     0.5812     0.1751         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.859      0.818      0.863      0.422\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     23/100      10.7G     0.6087     0.5872     0.1826         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.832      0.818      0.863      0.427\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     24/100      10.7G       0.59     0.5725     0.1731         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.856      0.803      0.855      0.412\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     25/100      10.7G     0.5937     0.5807     0.1746         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.847      0.804      0.856      0.435\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     26/100      10.7G     0.5805     0.5677     0.1709         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.826      0.821      0.855      0.429\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     27/100      10.7G     0.5845      0.567     0.1705         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.869       0.83      0.889      0.443\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     28/100      10.7G     0.5612     0.5564     0.1616         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.867      0.824      0.881      0.449\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     29/100      10.7G     0.5724      0.556     0.1674         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.856      0.818      0.871      0.443\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     30/100      10.7G     0.5549     0.5519     0.1628         20        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.872      0.832      0.883      0.445\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     31/100      10.7G     0.5679     0.5508     0.1659         26        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.877      0.825      0.875      0.426\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     32/100      10.7G     0.5576      0.538     0.1581         29        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.893      0.824      0.902      0.469\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     33/100      10.7G     0.5511     0.5462     0.1586         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.876      0.838      0.889      0.466\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     34/100      10.7G     0.5575     0.5522     0.1598         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.876       0.84      0.885      0.467\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     35/100      10.7G     0.5511     0.5341     0.1593         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.852      0.851      0.893      0.465\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     36/100      10.7G     0.5301     0.5359      0.148         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.882       0.85      0.892      0.475\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     37/100      10.7G     0.5413     0.5326     0.1497         26        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.855      0.862      0.894      0.462\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     38/100      10.7G     0.5228     0.5321      0.144         26        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.892      0.843      0.896      0.467\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     39/100      10.7G     0.5275     0.5356     0.1496         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.855      0.859      0.885      0.476\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     40/100      10.7G     0.5186     0.5211     0.1446         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.872      0.848      0.898      0.477\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     41/100      10.7G     0.5224     0.5353     0.1496         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.87       0.84      0.888      0.461\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     42/100      10.7G     0.5197     0.5188      0.147         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.886      0.833      0.884      0.452\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     43/100      10.7G      0.521     0.5196     0.1424         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.873      0.871      0.901      0.475\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     44/100      10.7G     0.5276     0.5172     0.1475         27        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.879      0.869      0.902      0.484\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     45/100      10.6G     0.5154     0.5107     0.1427         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.883       0.87      0.915      0.491\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     46/100      10.7G     0.5207     0.5126      0.149         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.868      0.869      0.897      0.476\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     47/100      10.7G     0.5092     0.5101     0.1384         50        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.861      0.877       0.89      0.477\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     48/100      10.7G     0.5108     0.5107     0.1432         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.88      0.869      0.908      0.488\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     49/100      10.7G     0.4982     0.4959     0.1377         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.886      0.868      0.902       0.48\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     50/100      10.7G      0.502     0.5021     0.1388         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        0.9       0.86      0.908      0.494\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     51/100      10.7G     0.4971     0.5067     0.1402         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.888      0.863      0.915      0.495\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     52/100      10.7G     0.4951     0.4983     0.1336         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.89      0.877       0.91      0.492\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     53/100      10.7G     0.4899     0.5058     0.1336         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.899      0.877      0.914      0.491\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     54/100      10.7G      0.484     0.4984     0.1346         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.879       0.88      0.908       0.49\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     55/100      10.7G     0.4678     0.4995     0.1263         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.892      0.876      0.912      0.491\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     56/100      10.7G     0.4801      0.496     0.1351         29        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.895      0.868      0.903      0.492\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     57/100      10.6G     0.4761     0.4871     0.1306         29        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.897      0.878      0.915      0.507\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     58/100      10.7G     0.4667     0.4832     0.1263         44        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.912      0.858      0.913      0.492\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     59/100      10.7G     0.4709       0.49     0.1312         49        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.891      0.884      0.913      0.506\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     60/100      10.7G     0.4649      0.478      0.128         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.891      0.895      0.914      0.492\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     61/100      10.7G     0.4664     0.4849     0.1291         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.902      0.894      0.922      0.505\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     62/100      10.7G     0.4579     0.4758     0.1234         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.901      0.891      0.924      0.513\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     63/100      10.6G     0.4663     0.4797     0.1278         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.897      0.881      0.915       0.51\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     64/100      10.7G     0.4506     0.4738     0.1231         35        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.887      0.887      0.918      0.512\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     65/100      10.7G     0.4599     0.4689      0.124         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.917      0.878      0.926      0.512\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     66/100      10.7G     0.4572     0.4777     0.1255         27        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.909      0.882      0.927      0.518\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     67/100      10.7G     0.4427     0.4745     0.1204         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.911      0.886      0.925      0.523\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     68/100      10.7G     0.4547     0.4711     0.1234         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.88      0.909      0.915      0.499\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     69/100      10.7G     0.4409      0.469     0.1152         52        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.906      0.887      0.919      0.517\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     70/100      10.7G     0.4348     0.4608     0.1158         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.896      0.892       0.92      0.512\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     71/100      10.7G     0.4412     0.4665     0.1184         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.89      0.897      0.916      0.519\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     72/100      10.7G     0.4458     0.4684     0.1199         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        0.9      0.898      0.919      0.521\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     73/100      10.6G     0.4304     0.4565     0.1164         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.909       0.89      0.918      0.523\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     74/100      10.7G     0.4363     0.4559     0.1147         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.883      0.909       0.92      0.506\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     75/100      10.7G     0.4303     0.4548     0.1147         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.903      0.887      0.922      0.516\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     76/100      10.7G     0.4299     0.4525     0.1128         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.907      0.889      0.925      0.532\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     77/100      10.7G     0.4214     0.4448     0.1132         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.894      0.907      0.929      0.533\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     78/100      10.7G      0.418     0.4496     0.1134         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.882      0.907      0.927      0.514\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     79/100      10.6G     0.4134     0.4427     0.1086         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.897      0.897      0.917      0.511\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     80/100      10.7G     0.4158     0.4426     0.1086         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462        0.9      0.895      0.925      0.525\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     81/100      10.7G     0.4244     0.4416     0.1127         25        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.899      0.906      0.927      0.525\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     82/100      10.7G     0.4132     0.4425     0.1107         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.907      0.892      0.925      0.521\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     83/100      10.7G     0.4146     0.4395     0.1067         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.901      0.891      0.916      0.518\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     84/100      10.6G     0.4115     0.4375     0.1091         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.903        0.9       0.93      0.542\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     85/100      10.7G     0.4025     0.4449     0.1074         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.906      0.905      0.928      0.542\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     86/100      10.7G     0.4025     0.4383     0.1054         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.916      0.906      0.925      0.539\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     87/100      10.7G     0.4057     0.4367      0.107         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.902      0.902      0.927      0.535\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     88/100      10.7G     0.3871     0.4333     0.1025         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.916      0.904      0.928      0.539\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     89/100      10.7G     0.4084     0.4366     0.1089         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.919      0.899       0.93       0.54\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     90/100      10.7G      0.383     0.4276    0.09725         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.918        0.9      0.932      0.537\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     91/100      10.7G     0.2915     0.3559    0.07163         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:38<00:00,  1.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.908      0.918      0.937      0.553\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     92/100      10.6G     0.2885     0.3473    0.07022         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462       0.91      0.926      0.943      0.559\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     93/100      10.7G     0.2854     0.3457    0.06963         21        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.924      0.913      0.942      0.557\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     94/100      10.7G     0.2767     0.3378     0.0675         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.908      0.925       0.94      0.559\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     95/100      10.7G     0.2745     0.3361    0.06663         14        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.931      0.905      0.939      0.561\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     96/100      10.7G      0.271     0.3345    0.06611         23        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.932      0.906       0.94      0.561\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     97/100      10.7G     0.2649     0.3306    0.06429         31        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.916      0.922      0.937      0.562\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     98/100      10.7G      0.269     0.3298    0.06549         26        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.927      0.908      0.937      0.565\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     99/100      10.6G     0.2616     0.3263    0.06277         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:37<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.918      0.917      0.938      0.566\n\n      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/Context.cpp:71.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    100/100      10.7G     0.2592     0.3244    0.06356         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:36<00:00,  1.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:13<00:00,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.917      0.922      0.939      0.563\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n100 epochs completed in 4.840 hours.\nOptimizer stripped from runs/detect/Svarka2/weights/last.pt, 135.4MB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSvarka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflipud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43merasing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtranslate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:674\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:199\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:467\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mplots:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_metrics()\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    469\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:162\u001b[0m, in \u001b[0;36mBaseTrainer.run_callbacks\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(event, []):\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/callbacks/wb.py:142\u001b[0m, in \u001b[0;36mon_train_end\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m curve_name, curve_values \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mcurves, trainer\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mcurves_results):\n\u001b[1;32m    141\u001b[0m     x, y, x_title, y_title \u001b[38;5;241m=\u001b[39m curve_values\n\u001b[0;32m--> 142\u001b[0m     \u001b[43m_plot_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurves/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurve_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurve_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m wb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mfinish()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/callbacks/wb.py:87\u001b[0m, in \u001b[0;36m_plot_curve\u001b[0;34m(x, y, names, id, title, x_title, y_title, num_x, only_mean)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Create arrays for logging\u001b[39;00m\n\u001b[1;32m     86\u001b[0m x_log \u001b[38;5;241m=\u001b[39m x_new\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 87\u001b[0m y_log \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_mean:\n\u001b[1;32m     90\u001b[0m     table \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39mTable(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(x_log, y_log)), columns\u001b[38;5;241m=\u001b[39m[x_title, y_title])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:1599\u001b[0m, in \u001b[0;36minterp\u001b[0;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     xp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((xp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mperiod, xp, xp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mperiod))\n\u001b[1;32m   1597\u001b[0m     fp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], fp, fp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: object of too small depth for desired array"],"ename":"ValueError","evalue":"object of too small depth for desired array","output_type":"error"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"model = RTDETR('/kaggle/working/runs/detect/Svarka2/weights/last.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:01:04.457235Z","iopub.execute_input":"2024-06-15T17:01:04.457608Z","iopub.status.idle":"2024-06-15T17:01:04.749549Z","shell.execute_reply.started":"2024-06-15T17:01:04.457578Z","shell.execute_reply":"2024-06-15T17:01:04.748404Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.val()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:01:15.948272Z","iopub.execute_input":"2024-06-15T17:01:15.949036Z","iopub.status.idle":"2024-06-15T17:01:53.756940Z","shell.execute_reply.started":"2024-06-15T17:01:15.949006Z","shell.execute_reply":"2024-06-15T17:01:53.754849Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.32 üöÄ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\nrt-detr-x summary: 642 layers, 65477711 parameters, 0 gradients, 222.5 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels/val... 434 images, 32 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 434/434 [00:00<00:00, 716.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/yolosvarka-augmented/YOLOSvarka_augmented/labels is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:30<00:00,  1.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all        434       1462      0.916      0.922      0.939      0.564\n                   adj        145        520      0.883      0.898      0.886      0.438\n                   int        192        250      0.937       0.92      0.934       0.56\n                   geo        281        380      0.948      0.926      0.964        0.6\n                   pro        118        189      0.924      0.947      0.968      0.586\n                   non         91        123      0.889      0.919      0.943      0.636\nSpeed: 0.2ms preprocess, 62.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val2\u001b[0m\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0, 1, 2, 3, 4])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e0f7af4c850>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,  0.00033732,  0.00016866,           0],\n       [          1,           1,           1, ...,   0.0012516,  0.00062582,           0],\n       [          1,           1,           1, ...,   0.0028186,   0.0014093,           0],\n       [          1,           1,           1, ...,    0.047957,    0.047957,           0],\n       [          1,           1,           1, ...,   0.0051716,   0.0025858,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.014152,    0.014152,    0.014152, ...,           0,           0,           0],\n       [   0.024693,    0.024693,    0.024693, ...,           0,           0,           0],\n       [   0.029203,    0.029203,    0.029203, ...,           0,           0,           0],\n       [   0.038749,    0.038749,    0.038749, ...,           0,           0,           0],\n       [   0.041133,    0.041133,    0.041133, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0071285,   0.0071285,   0.0071285, ...,           1,           1,           1],\n       [   0.012504,    0.012504,    0.012504, ...,           1,           1,           1],\n       [    0.01482,     0.01482,     0.01482, ...,           1,           1,           1],\n       [   0.019757,    0.019757,    0.019757, ...,           1,           1,           1],\n       [   0.021002,    0.021002,    0.021002, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.95769,     0.95769,     0.95769, ...,           0,           0,           0],\n       [       0.98,        0.98,        0.98, ...,           0,           0,           0],\n       [    0.98947,     0.98947,     0.98947, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [    0.99187,     0.99187,     0.99187, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.6012929195030294\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.43755,     0.55962,         0.6,     0.58567,     0.63602])\nnames: {0: 'adj', 1: 'int', 2: 'geo', 3: 'pro', 4: 'non'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.916053275213214, 'metrics/recall(B)': 0.9220363693264849, 'metrics/mAP50(B)': 0.9389974810079851, 'metrics/mAP50-95(B)': 0.5637701904469232, 'fitness': 0.6012929195030294}\nsave_dir: PosixPath('runs/detect/val2')\nspeed: {'preprocess': 0.21829912739415322, 'inference': 62.004726053932295, 'loss': 0.0009465327460644981, 'postprocess': 0.43192931583949495}\ntask: 'detect'"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r /kaggle/working/aaa.zip /kaggle/working/runs/detect/Svarka2","metadata":{"papermill":{"duration":3.007404,"end_time":"2024-06-15T08:04:26.499896","exception":false,"start_time":"2024-06-15T08:04:23.492492","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T17:29:05.123133Z","iopub.execute_input":"2024-06-15T17:29:05.123753Z","iopub.status.idle":"2024-06-15T17:29:14.800466Z","shell.execute_reply.started":"2024-06-15T17:29:05.123722Z","shell.execute_reply":"2024-06-15T17:29:14.799273Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"updating: kaggle/working/runs/detect/Svarka2/ (stored 0%)\nupdating: kaggle/working/runs/detect/Svarka2/aaa.zip (stored 0%)\nupdating: kaggle/working/runs/detect/Svarka2/args.yaml (deflated 51%)\nupdating: kaggle/working/runs/detect/Svarka2/results.png (deflated 9%)\nupdating: kaggle/working/runs/detect/Svarka2/events.out.tfevents.1718452892.2fe495cdfae3.34.1 (deflated 94%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch2.jpg (deflated 1%)\nupdating: kaggle/working/runs/detect/Svarka2/labels.jpg (deflated 25%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch1.jpg (deflated 1%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch18001.jpg (deflated 3%)\nupdating: kaggle/working/runs/detect/Svarka2/labels_correlogram.jpg (deflated 34%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch0.jpg (deflated 2%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch18002.jpg (deflated 2%)\nupdating: kaggle/working/runs/detect/Svarka2/results.csv (deflated 85%)\nupdating: kaggle/working/runs/detect/Svarka2/directory.zip (stored 0%)\nupdating: kaggle/working/runs/detect/Svarka2/train_batch18000.jpg (deflated 2%)\nupdating: kaggle/working/runs/detect/Svarka2/weights/ (stored 0%)\nupdating: kaggle/working/runs/detect/Svarka2/weights/last.pt (deflated 8%)\n  adding: kaggle/working/runs/detect/Svarka2/submission.csv (deflated 63%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/ (stored 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/ (stored 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/ (stored 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch2_pred.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/PR_curve.png (deflated 14%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch1_labels.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/R_curve.png (deflated 11%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/P_curve.png (deflated 11%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch0_pred.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch1_pred.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/F1_curve.png (deflated 9%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/confusion_matrix.png (deflated 31%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch0_labels.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/val_batch2_labels.jpg (deflated 0%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val2/confusion_matrix_normalized.png (deflated 28%)\n  adding: kaggle/working/runs/detect/Svarka2/runs/detect/val/ (stored 0%)\n","output_type":"stream"}]}]}